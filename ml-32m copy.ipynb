{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "095669c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, RankingEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "391cc5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.1\n",
      "Log Level set to ALL\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieRecs\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ALL\")\n",
    "print(\"Log Level set to ALL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf55ed",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a3a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_cols = ['userId', 'movieId', 'rating', 'timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfecea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv('ml-32m/ratings.csv', schema=StructType([\n",
    "    StructField('userId', IntegerType(), True),\n",
    "    StructField('movieId', IntegerType(), True),\n",
    "    StructField('rating', FloatType(), True),\n",
    "    StructField('timestamp', IntegerType(), True)\n",
    "])).toDF(*ratings_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f526e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df_pd = pd.read_csv('ml-32m/ratings.csv', sep=',', header=0, \n",
    "                            dtype={'userId': np.int32, 'movieId': np.int32, 'rating': np.float32, 'timestamp': np.int64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e86723",
   "metadata": {},
   "source": [
    "PySpark reads 32m rows way faster than Pandas. This shows PySpark's superiority in handling large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2188ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cols = ['movieId', 'title', 'genres']\n",
    "movies_df = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv('ml-32m/movies.csv', schema=StructType([\n",
    "    StructField('movieId', IntegerType(), True),\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('genres', StringType(), True)\n",
    "])).toDF(*movies_cols)\n",
    "\n",
    "tags_cols = ['userId', 'movieId', 'tag', 'timestamp']\n",
    "tags_df = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv('ml-32m/tags.csv', schema=StructType([\n",
    "    StructField('userId', IntegerType(), True),\n",
    "    StructField('movieId', IntegerType(), True),\n",
    "    StructField('tag', StringType(), True),\n",
    "    StructField('timestamp', IntegerType(), True)\n",
    "])).toDF(*tags_cols)\n",
    "\n",
    "links_cols = ['movieId', 'imdbId', 'tmdbId']\n",
    "links_df = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv('ml-32m/links.csv', schema=StructType([\n",
    "    StructField('movieId', IntegerType(), True),\n",
    "    StructField('imdbId', IntegerType(), True),\n",
    "    StructField('tmdbId', IntegerType(), True)\n",
    "])).toDF(*links_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "173f2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = ratings_df.withColumn(\"datetime\", F.col(\"timestamp\").cast(\"timestamp\"))\n",
    "tags_df = tags_df.withColumn(\"datetime\", F.col(\"timestamp\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "167ab994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- imdbId: integer (nullable = true)\n",
      " |-- tmdbId: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(movies_df.printSchema())\n",
    "print(ratings_df.printSchema())\n",
    "print(tags_df.printSchema())\n",
    "print(links_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4974f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes\n",
    "merged_df = ratings_df.join(movies_df, on='movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d77ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08b53261",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = ratings_df.withColumn(\"year\", F.year(\"datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dafd9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df.withColumn(\"releaseYear\", F.regexp_extract(\"title\", r'\\((\\d{4})\\)$', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc9ecb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_genre_df = movies_df.withColumn(\"genre\", F.explode(F.split(F.col(\"genres\"), \"\\\\|\")))\n",
    "movies_genre_df = movies_genre_df.drop(\"genres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27ee6db",
   "metadata": {},
   "source": [
    "# 3. Modeling\n",
    "\n",
    "RQs:\n",
    "\n",
    "RQ 1: How does the choice of feedback signal (Explicit Ratings vs. Implicit Binary) impact the Top-N ranking quality of ALS when evaluated under a realistic retrieval protocol?\n",
    "\n",
    "RQ 2: Does introducing non-linearity via Neural Collaborative Filtering (NCF/AutoRec) provide a statistically significant improvement over linear ALS on sparse data?\n",
    "\n",
    "RQ 3: Does incorporating sequential dynamics (SASRec) outperform static collaborative filtering (ALS/NCF) for predicting immediate next-item interactions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a01bd8",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f47c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset Size: 988745 rows\n"
     ]
    }
   ],
   "source": [
    "# Taking last 1 year of data\n",
    "\n",
    "max_datetime = ratings_df.agg(F.max(\"datetime\")).collect()[0][0]\n",
    "cutoff_datetime = max_datetime.replace(year=max_datetime.year - 1)\n",
    "ratings_small_df = ratings_df.filter(F.col(\"datetime\") >= F.lit(cutoff_datetime))\n",
    "print(f\"New Dataset Size: {ratings_small_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "389d7bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data at datetime: 2023-09-10 18:00:00\n",
      "Train\n",
      "Interaction: 889317\n",
      "Users: 9562\n",
      "Items: 38046\n",
      "\n",
      "Test\n",
      "Interaction: 25305\n",
      "Users: 2118\n",
      "Items: 8078\n"
     ]
    }
   ],
   "source": [
    "quantiles = ratings_small_df.stat.approxQuantile(\"timestamp\", [0.9], 0.001)\n",
    "split_timestamp = quantiles[0]\n",
    "print(f\"Splitting data at datetime: {datetime.fromtimestamp(split_timestamp)}\")\n",
    "\n",
    "# 2. Define Train & Cache Immediately\n",
    "train_df = ratings_small_df.filter(F.col(\"timestamp\") < split_timestamp)\n",
    "train_df.cache()\n",
    "\n",
    "# 3. OPTIMIZATION A: Single-Pass Metrics Calculation\n",
    "# Instead of 3 separate count() calls, we get all metrics in 1 job.\n",
    "# This triggers the cache for 'train_df' efficiently.\n",
    "train_metrics = train_df.agg(\n",
    "    F.count(\"*\").alias(\"interactions\"),\n",
    "    F.countDistinct(\"userId\").alias(\"users\"),\n",
    "    F.countDistinct(\"movieId\").alias(\"items\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"Train\")\n",
    "print(f\"Interaction: {train_metrics['interactions']}\")\n",
    "print(f\"Users: {train_metrics['users']}\")\n",
    "print(f\"Items: {train_metrics['items']}\")\n",
    "\n",
    "# 4. Define Distinct Lists for Filtering\n",
    "# Since train_df is now cached, these are computed quickly.\n",
    "train_users_distinct = train_df.select(\"userId\").distinct()\n",
    "train_items_distinct = train_df.select(\"movieId\").distinct()\n",
    "\n",
    "# 5. Define Test Raw\n",
    "test_df_raw = ratings_small_df.filter(F.col(\"timestamp\") >= split_timestamp)\n",
    "\n",
    "# 6. OPTIMIZATION B: Broadcast Join\n",
    "# Since distinct users/items are usually small compared to interaction data,\n",
    "# we 'broadcast' them to all nodes to avoid a massive Shuffle Sort Merge Join.\n",
    "test_df_clean = test_df_raw \\\n",
    "    .join(F.broadcast(train_users_distinct), on=\"userId\", how=\"inner\") \\\n",
    "    .join(F.broadcast(train_items_distinct), on=\"movieId\", how=\"inner\")\n",
    "\n",
    "# 7. OPTIMIZATION C: Single-Pass Test Metrics\n",
    "# Again, get all 3 test metrics in just 1 job.\n",
    "test_metrics = test_df_clean.agg(\n",
    "    F.count(\"*\").alias(\"interactions\"),\n",
    "    F.countDistinct(\"userId\").alias(\"users\"),\n",
    "    F.countDistinct(\"movieId\").alias(\"items\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\nTest\")\n",
    "print(f\"Interaction: {test_metrics['interactions']}\")\n",
    "print(f\"Users: {test_metrics['users']}\")\n",
    "print(f\"Items: {test_metrics['items']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db780d",
   "metadata": {},
   "source": [
    "Train data is 99.77% sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b2d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For implicit feedback, set rating threshold\n",
    "rating_threshold = 3.5\n",
    "train_i_df = train_df.withColumn(\"implicit_label\", F.when(F.col(\"rating\") >= rating_threshold, 1.0).otherwise(0.0))\n",
    "test_i_df_clean = test_df_clean.withColumn(\"implicit_label\", F.when(F.col(\"rating\") >= rating_threshold, 1.0).otherwise(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a0151",
   "metadata": {},
   "source": [
    "# 3. Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d5bda6",
   "metadata": {},
   "source": [
    "RQ 1: How does the choice of feedback signal (Explicit Ratings vs. Implicit Binary) impact the Top-N ranking quality of ALS when evaluated under a realistic retrieval protocol?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee85394",
   "metadata": {},
   "source": [
    "### ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4efa6894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new ALS model...\n"
     ]
    }
   ],
   "source": [
    "# Model on explicit feedback\n",
    "\n",
    "ALS_E_MODEL_PATH = \"models/als_e_model\"\n",
    "\n",
    "if os.path.exists(ALS_E_MODEL_PATH):\n",
    "    print(\"Loading existing explicit ALS model...\")\n",
    "    als_e_model = ALSModel.load(ALS_E_MODEL_PATH)\n",
    "else:\n",
    "    print(\"Training new ALS model...\")\n",
    "    als = ALS(rank=30, \n",
    "            maxIter=10, \n",
    "            regParam=0.1, \n",
    "            numUserBlocks=10, \n",
    "            numItemBlocks=10, \n",
    "            implicitPrefs=False,\n",
    "            alpha=1.0,\n",
    "            userCol='userId',\n",
    "            itemCol='movieId',\n",
    "            seed=42,\n",
    "            ratingCol='rating',\n",
    "            nonnegative=False,\n",
    "            checkpointInterval=10,\n",
    "            coldStartStrategy=\"drop\")\n",
    "    als_e_model = als.fit(train_df)\n",
    "    als_e_model.save(ALS_E_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7e484cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new ALS model...\n"
     ]
    }
   ],
   "source": [
    "# Model on implicit feedback\n",
    "\n",
    "ALS_I_MODEL_PATH = \"models/als_i_model\"\n",
    "\n",
    "if os.path.exists(ALS_I_MODEL_PATH):\n",
    "    print(\"Loading existing implicit ALS model...\")\n",
    "    als_i_model = ALSModel.load(ALS_I_MODEL_PATH)\n",
    "else:\n",
    "    print(\"Training new ALS model...\")\n",
    "    als = ALS(rank=30, \n",
    "            maxIter=10, \n",
    "            regParam=0.1, \n",
    "            numUserBlocks=10, \n",
    "            numItemBlocks=10, \n",
    "            implicitPrefs=True,\n",
    "            alpha=1.0,\n",
    "            userCol='userId',\n",
    "            itemCol='movieId',\n",
    "            seed=42,\n",
    "            ratingCol='rating',\n",
    "            nonnegative=False,\n",
    "            checkpointInterval=10,\n",
    "            coldStartStrategy=\"drop\")\n",
    "    als_i_model = als.fit(train_i_df)\n",
    "    als_i_model.save(ALS_I_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1247278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_count_df = train_df.groupBy(\"userId\").agg(F.count(\"movieId\").alias(\"seenItemsCount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8f022e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|userId|seenItemsCount|\n",
      "+------+--------------+\n",
      "|103013|          2991|\n",
      "|108412|          2917|\n",
      "| 87324|          2741|\n",
      "|161180|          2720|\n",
      "|  1668|          2544|\n",
      "+------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "user_movie_count_df.sort(F.col(\"seenItemsCount\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b8a3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_ndcg_at_k_full_ranking(test_df, train_df, model, k=10):\n",
    "    userid_test_df = test_df.select('userId').distinct()\n",
    "    raw_recs = model.recommendForUserSubset(userid_test_df, 3500)\n",
    "    exploded_recs = raw_recs.select(\"userId\", F.explode(\"recommendations\").alias(\"rec\")).select(\"userId\", F.col(\"rec.movieId\").alias(\"movieId\"), F.col(\"rec.rating\").alias(\"prediction\"))\n",
    "    new_item_recs = exploded_recs.join(train_df, on=[\"userId\", \"movieId\"], how=\"left_anti\")\n",
    "    final_recs = new_item_recs.withColumn(\"rank\", F.row_number().over(\n",
    "        Window.partitionBy(\"userId\").orderBy(F.desc(\"prediction\"))\n",
    "    )).filter(F.col(\"rank\") <= k) \\\n",
    "    .groupBy(\"userId\") \\\n",
    "    .agg(F.collect_list(F.col(\"movieId\").cast(\"double\")).alias(\"predicted_movieIds\"))\n",
    "    ground_truth_per_user = test_df.filter(F.col(\"rating\") >= 3.5) \\\n",
    "        .groupBy('userId') \\\n",
    "        .agg(F.collect_set(F.col('movieId').cast(\"double\")).alias('liked_movieIds'))\n",
    "    recommendations_with_truth = final_recs.join(ground_truth_per_user, on='userId', how='inner')\n",
    "    \n",
    "    # hit@k\n",
    "    recommendations_with_truth = recommendations_with_truth.withColumn(\"hit\", F.expr(\"array_intersect(predicted_movieIds, liked_movieIds)\")) \\\n",
    "        .withColumn(\"hit\", F.when(F.size(F.col(\"hit\")) > 0, 1.0).otherwise(0.0))\n",
    "    hit_at_k = recommendations_with_truth.agg(F.avg(\"hit\")).collect()[0][0]\n",
    "    \n",
    "    # precision@k\n",
    "    p_evaluator = RankingEvaluator(\n",
    "        metricName=\"precisionAtK\", \n",
    "        k=k, \n",
    "        predictionCol=\"predicted_movieIds\", \n",
    "        labelCol=\"liked_movieIds\"\n",
    "    )\n",
    "    \n",
    "    # recall@k\n",
    "    r_evaluator = RankingEvaluator(\n",
    "        metricName=\"recallAtK\", \n",
    "        k=k, \n",
    "        predictionCol=\"predicted_movieIds\", \n",
    "        labelCol=\"liked_movieIds\"\n",
    "    )\n",
    "\n",
    "    precision_at_k = p_evaluator.evaluate(recommendations_with_truth)\n",
    "    recall_at_k = r_evaluator.evaluate(recommendations_with_truth)\n",
    "    return precision_at_k, recall_at_k, hit_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff3dc09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explicit ALS Model - Test Precision@10: 0.0001, Recall@10: 0.0001, Hit@10: 0.0011\n"
     ]
    }
   ],
   "source": [
    "test_e_precision_fr, test_e_recall_fr, test_e_hit_fr = precision_recall_ndcg_at_k_full_ranking(test_df_clean, train_df, als_e_model, k=10)\n",
    "print(f\"Explicit ALS Model - Test Precision@10: {test_e_precision_fr:.4f}, Recall@10: {test_e_recall_fr:.4f}, Hit@10: {test_e_hit_fr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfedb435",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_i_precision_fr, test_i_recall_fr, test_i_hit_fr = precision_recall_ndcg_at_k_full_ranking(test_df_clean, train_df, als_i_model, k=10)\n",
    "print(f\"Implicit ALS Model - Test Precision@10: {test_i_precision_fr:.4f}, Recall@10: {test_i_recall_fr:.4f}, Hit@10: {test_i_hit_fr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
